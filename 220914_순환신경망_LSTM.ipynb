{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNOG9ixl7dwrgg/6RNxK3dB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junseokee/Deep_Learning/blob/main/220914_%EC%88%9C%ED%99%98%EC%8B%A0%EA%B2%BD%EB%A7%9D_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM(Long Short-Term Memory)\n",
        "\n",
        "- 단순한 순환 신경망의 문제점\n",
        "  - 시점이 흐를수록 지속해서 기억하지 못한다.\n",
        "  - 그래디언트 손실 문제\n",
        "\n",
        "- 이를 해결하기 위해 1997년 고안된 방벅\n",
        "  - LSTM\n",
        "\n",
        "- LSTM의 핵심은 정보를 여러 시점에 걸쳐 나르는 장치가 추가되었다는 것\n",
        "  - 그래디언트를 보존할 수 있어 그래디언트 손실 문제가 발생하지 않도록 도와줌\n",
        "  - 아래 그림에서 C로 표현된 Cell State h는 hidden state\n",
        "  - 정보 나르는 것을 도와주는 3개의 게이트 forget, input, output"
      ],
      "metadata": {
        "id": "XphJoNW_vC89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 로이터 데이터셋\n",
        "- reuters 데이터뎃\n",
        "  - iMDB 데이터셋과 유사\n",
        "  - 총 11258개의 뉴스기사가 46개 카테고리로 이루어져있음\n",
        "\n",
        "데이터 다운로드 -> 문장 길이 동일하게 맞추기 -> "
      ],
      "metadata": {
        "id": "D8pywTMHKfq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Flatten, Dense, Embedding\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim = num_words, output_dim = 64))\n",
        "# SimpleRNN 층을 첫 번째 층으로 사용하는 경우,\n",
        "# 반드시 input_shape를 명시해주어야 합니다.\n",
        "# 새로운 인자 3개가 사용되었습니다.\n",
        "# return_sequences, dropout, recurrent_dropout\n",
        "model.add(LSTM(64, return_sequences =True, dropout = 0.15, input_dim = num_words\n",
        "                    output_dim = 64, recurrent_dropout = 0.15))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(46,activation = 'softmax'))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
        "              metrics = ['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(pad_X_train, y_train,\n",
        "                    batch_size = 32, epochs = 15,\n",
        "                    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "yGFGnbdhLGg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conv1D\n",
        "- 시퀀스 데이터도 컨볼루션 방법을 통해 특징을 추출할 수 있다.\n",
        "\n",
        "- Conv1D층\n",
        "  - 1차원 형태의 컨볼루션 필터를 가지는 1D 컨볼루션층\n",
        "  - LSTM과 혼합하여 사용되는 등 순환 신경망과 함께 사용됨\n",
        "\n",
        "- Conv2D: 2D 형태를 가진 컨볼루션 필터로 컨볼루션 연사늘 수행하여 이미지 특징을 추출\n",
        "- Conv1D 1D 형태를 가진 컨볼루션 필터로 컨볼루션 연산을 수행하여 시퀀스 데이터의 특징을 추출\n",
        "\n",
        "- 나는 좋다라는 문맥을 문장의 앞부분에서 인식했다면, 동일하게 뒷부분애서도 인식 할 수있음"
      ],
      "metadata": {
        "id": "H4hgQyI-LvED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "# 1~10,000 빈도 순위에 해당하는 단어만 사용합니다.\n",
        "num_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = num_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkQi75WSLu0D",
        "outputId": "670fa0b2-45ab-4d09-e94b-52f991ba39e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = 500\n",
        "\n",
        "pad_X_train = pad_sequences(X_train,maxlen=max_len,)\n",
        "pad_X_test = pad_sequences(X_test,maxlen=max_len,)"
      ],
      "metadata": {
        "id": "jZjd2d_KN42v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "컨볼루션 1D층은 배치사이즈, 타임스텝, 채널) 형태를 입력으로 받으며, (배치사이즈,타임스텝,필터) 형태를 출력\n",
        "- 컨볼루션 층ㅇ에서 특이하게 5 X 7 필터를 사용\n"
      ],
      "metadata": {
        "id": "nXZz5toHMe0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, Flatten, Dense, Embedding, Conv1D, MaxPooling1D,GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim = num_words, output_dim = 32, input_length = max_len))\n",
        "\n",
        "model.add(Conv1D(32, 7, activation = 'relu'))\n",
        "model.add(MaxPooling1D(7))\n",
        "model.add(Conv1D(32, 5, activation = 'relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1,activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
        "              metrics = ['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(pad_X_train, y_train,\n",
        "                    batch_size = 32, epochs = 30,\n",
        "                    validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvopbutAM1lf",
        "outputId": "d8de723b-8320-4c11-8e08-17c60b5c0891"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 500, 32)           320000    \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 494, 32)           7200      \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPooling  (None, 70, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 66, 32)            5152      \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPooling  (None, 13, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 332,385\n",
            "Trainable params: 332,385\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "625/625 [==============================] - 15s 4ms/step - loss: 0.4591 - acc: 0.7576 - val_loss: 0.3201 - val_acc: 0.8648\n",
            "Epoch 2/30\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2340 - acc: 0.9069 - val_loss: 0.3196 - val_acc: 0.8684\n",
            "Epoch 3/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1366 - acc: 0.9508 - val_loss: 0.3758 - val_acc: 0.8676\n",
            "Epoch 4/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0653 - acc: 0.9794 - val_loss: 0.4861 - val_acc: 0.8658\n",
            "Epoch 5/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0224 - acc: 0.9949 - val_loss: 0.6443 - val_acc: 0.8600\n",
            "Epoch 6/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0060 - acc: 0.9994 - val_loss: 0.7032 - val_acc: 0.8606\n",
            "Epoch 7/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0027 - acc: 0.9998 - val_loss: 0.7737 - val_acc: 0.8640\n",
            "Epoch 8/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0016 - acc: 0.9999 - val_loss: 0.8213 - val_acc: 0.8638\n",
            "Epoch 9/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.9092 - val_acc: 0.8532\n",
            "Epoch 10/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.9340 - val_acc: 0.8618\n",
            "Epoch 11/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0420 - acc: 0.9859 - val_loss: 0.8075 - val_acc: 0.8554\n",
            "Epoch 12/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0136 - acc: 0.9954 - val_loss: 0.8601 - val_acc: 0.8564\n",
            "Epoch 13/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.0024 - acc: 0.9996 - val_loss: 0.9166 - val_acc: 0.8572\n",
            "Epoch 14/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 4.3594e-04 - acc: 1.0000 - val_loss: 0.9592 - val_acc: 0.8604\n",
            "Epoch 15/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 2.4458e-04 - acc: 1.0000 - val_loss: 0.9829 - val_acc: 0.8608\n",
            "Epoch 16/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 1.9871e-04 - acc: 1.0000 - val_loss: 1.0069 - val_acc: 0.8598\n",
            "Epoch 17/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 1.6990e-04 - acc: 1.0000 - val_loss: 1.0335 - val_acc: 0.8610\n",
            "Epoch 18/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 1.4798e-04 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.8618\n",
            "Epoch 19/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 1.3090e-04 - acc: 1.0000 - val_loss: 1.0871 - val_acc: 0.8618\n",
            "Epoch 20/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 1.1514e-04 - acc: 1.0000 - val_loss: 1.1126 - val_acc: 0.8620\n",
            "Epoch 21/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 1.0102e-04 - acc: 1.0000 - val_loss: 1.1405 - val_acc: 0.8616\n",
            "Epoch 22/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 8.7952e-05 - acc: 1.0000 - val_loss: 1.1705 - val_acc: 0.8608\n",
            "Epoch 23/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 7.5997e-05 - acc: 1.0000 - val_loss: 1.1994 - val_acc: 0.8608\n",
            "Epoch 24/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 6.5453e-05 - acc: 1.0000 - val_loss: 1.2298 - val_acc: 0.8604\n",
            "Epoch 25/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 5.4708e-05 - acc: 1.0000 - val_loss: 1.2608 - val_acc: 0.8598\n",
            "Epoch 26/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 4.5741e-05 - acc: 1.0000 - val_loss: 1.2932 - val_acc: 0.8596\n",
            "Epoch 27/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 3.8071e-05 - acc: 1.0000 - val_loss: 1.3262 - val_acc: 0.8604\n",
            "Epoch 28/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 3.1497e-05 - acc: 1.0000 - val_loss: 1.3604 - val_acc: 0.8604\n",
            "Epoch 29/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 2.5749e-05 - acc: 1.0000 - val_loss: 1.3960 - val_acc: 0.8602\n",
            "Epoch 30/30\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 2.0418e-05 - acc: 1.0000 - val_loss: 1.4319 - val_acc: 0.8602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqqndsvoN7JN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}